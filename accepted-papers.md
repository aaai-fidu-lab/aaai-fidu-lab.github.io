---
title: Accepted Papers
nav: true
---

# Accepted Papers

The following tiny papers have been accepted to the EvalEval Workshop at NeurIPS 2024:

## Oral Presentations

* **[Provocation: Who benefits from "inclusion" in Generative AI?](accepted_papers/EvalEval_24_Dalal.pdf)**  
  *Samantha Dalal, Siobhan Mackenzie Hall, Nari Johnson*

* **[(Mis)use of Nude Images in Machine Learning Research](accepted_papers/EvalEval_24_Arya.pdf)**  
  *Arshia Arya, Princessa Cintaqia, Deepak Kumar, Allison McDonald, Lucy Qin, Elissa M Redmiles*

* **[Evaluating Refusal](accepted_papers/EvalEval_24_Abramovich.pdf)**  
  *Shira Abramovich, Anna Ma*

* **[JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark](accepted_papers/EvalEval_24_Onohara.pdf)**  
  *Shota Onohara, Atsuyuki Miyai, Yuki Imajuku, Kazuki Egashira, Jeonghun Baek, Xiang Yue, Graham Neubig, Kiyoharu Aizawa*

* **[Critical human-AI use scenarios and interaction modes for societal impact evaluations](accepted_papers/EvalEval_24_Ibrahim.pdf)**  
  *Lujain Ibrahim, Saffron Huang, Lama Ahmad, Markus Anderljung*

* **[Cascaded to End-to-End: New Safety, Security, and Evaluation Questions for Audio Language Models](accepted_papers/EvalEval_24_He.pdf)**  
  *Luxi He, Xiangyu Qi, Inyoung Cheong, Prateek Mittal, Danqi Chen, Peter Henderson*

* **[GenAI Evaluation Maturity Framework (GEMF)](accepted_papers/EvalEval_24_Zhang.pdf)**  
  *Yilin Zhang, Frank J Kanayet*

* **[AIR-Bench 2024: Safety Evaluation Based on Risk Categories](accepted_papers/EvalEval_24_Klyman.pdf)**  
  *Kevin Klyman*

* **[Evaluating Generative AI Systems is a Social Science Measurement Challenge](accepted_papers/EvalEval_24_Wallach.pdf)**  
  *Hanna Wallach, Meera Desai, Nicholas J Pangakis, A. Feder Cooper, Angelina Wang, Solon Barocas, Alexandra Chouldechova, Chad Atalla, Su Lin Blodgett, Emily Corvi, P. Alex Dow, Jean Garcia-Gathright, Alexandra Olteanu, Stefanie Reed, Emily Sheng, Dan Vann, Jennifer Wortman Vaughan, Matthew Vogel, Hannah Washington, Abigail Z Jacobs.*

## Poster Presentations

* **[Evaluations Using Wikipedia without Data Contamination: From Trusting Articles to Trusting Edit Processes](accepted_papers/EvalEval_24_Kaffee.pdf)**  
  *Lucie-Aim√©e Kaffee, Isaac Johnson*

* **[Can Vision-Language Models Replace Human Annotators: A Case Study with CelebA Dataset](accepted_papers/EvalEval_24_Lu.pdf)**  
  *Haoming Lu, Feifei Zhong*

* **[Using Scenario-Writing for Identifying and Mitigating Impacts of Generative AI](accepted_papers/EvalEval_24_Kieslich.pdf)**  
  *Kimon Kieslich, Nicholas Diakopoulos, Natali Helberger*

* **[Troubling taxonomies in GenAI evaluation](accepted_papers/EvalEval_24_Berman.pdf)**  
  *Glen Berman, Ned Cooper, Wesley Deng, Ben Hutchinson*

* **[Is ETHICS about ethics? Evaluating the ETHICS benchmark](accepted_papers/EvalEval_24_Hancox-Li.pdf)**  
  *Leif Hancox-Li, Borhane Blili-Hamelin*

* **[Provocation on Expertise in Social Impact Evaluations for Generative AI (and Beyond)](accepted_papers/EvalEval_24_Kahn.pdf)**  
  *Zoe Kahn, Nitin Kohli*

* **[Rethinking CyberSecEval: An LLM-Aided Approach to Evaluation Critique](accepted_papers/EvalEval_24_Hariharan.pdf)**  
  *Suhas Hariharan, Zainab Ali Majid, Jaime Raldua Veuthey, Jacob Haimes*

* **[Contamination Report for Multilingual Benchmarks](accepted_papers/EvalEval_24_Ahuja.pdf)**  
  *Sanchit Ahuja, Varun Gumma, Sunayana Sitaram*

* **[Towards Leveraging News Media to Support Impact Assessment of AI Technologies](accepted_papers/EvalEval_24_Allaham.pdf)**  
  *Mowafak Allaham, Kimon Kieslich, Nicholas Diakopoulos*

* **[Motivations for Reframing Large Language Model Benchmarking for Legal Applications](accepted_papers/EvalEval_24_Ranjan.pdf)**  
  *Riya Ranjan, Megan Ma*

* **[A Framework for Evaluating LLMs Under Task Indeterminacy](accepted_papers/EvalEval_24_Guerdan.pdf)**  
  *Luke Guerdan, Hanna Wallach, Solon Barocas, Alexandra Chouldechova*

* **[Dimensions of Generative AI Evaluation Design](accepted_papers/EvalEval_24_Dow.pdf)**  
  *P. Alex Dow, Jennifer Wortman Vaughan, Solon Barocas, Chad Atalla, Alexandra Chouldechova, Hanna Wallach*

* **[Statistical Bias in Bias Benchmark Design](accepted_papers/EvalEval_24_Powers.pdf)**  
  *Hannah Powers, Ioana Baldini, Dennis Wei, Kristin Bennett*

* **[Rethinking Artistic Copyright Infringements in the Era of Text-to-Image Generative Models](accepted_papers/EvalEval_24_Moayeri.pdf)**  
  *Mazda Moayeri, Samyadeep Basu, Sriram Balasubramanian, Priyatham Kattakinda, Atoosa Chegini, Robert Brauneis, Soheil Feizi*

* **[Gaps Between Research and Practice When Measuring Representational Harms Caused by LLM-Based Systems](accepted_papers/EvalEval_24_Harvey.pdf)**  
  *Emma Harvey, Emily Sheng, Su Lin Blodgett, Alexandra Chouldechova, Jean Garcia-Gathright, Alexandra Olteanu, Hanna Wallach*

* **[Surveying Surveys: Surveys' Role in Evaluating AI's Labor Market Impact](accepted_papers/EvalEval_24_Solis.pdf)**  
  *Cassandra Duchan Solis*

* **[Fairness Dynamics During Training](accepted_papers/EvalEval_24_Patel.pdf)**  
  *Krishna Patel, Nivedha Sivakumar, Barry-John Theobald, Luca Zappella, Nicholas Apostoloff*

* **[Democratic Perspectives and Institutional Capture of Crowdsourced Evaluations](accepted_papers/EvalEval_24_sarin.pdf)**  
  *parth sarin, Michelle Bao*

* **[LLMs and Personalities: Inconsistencies Across Scales](accepted_papers/EvalEval_24_Tosato.pdf)**  
  *Tosato Tommaso, Lemay David, Mahmood Hegazy, Irina Rish, Guillaume Dumas*

* **[Assessing Bias in Metric Models for LLM Open-Ended Generation Bias Benchmarks](accepted_papers/EvalEval_24_Demchak.pdf)**  
  *Nathaniel Demchak, Xin Guan, Zekun Wu, Ziyi Xu, Adriano Koshiyama, Emre Kazim*
